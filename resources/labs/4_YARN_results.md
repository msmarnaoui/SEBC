# Change that I performed on script:
 * output of Teragen and Terasort:terasort output was overriding teragen output of the same test run  
 * The size of the teragen created file (from 5 Go to 1Ã  Go

# First Test: <strong> run without any parameter change</strong> 

Container memory :  512

* Teragen Time: 2m20 

* Terasort Time: 5m6

Container memory :  1024

* Teragen Time: 2M33 

* Terasort Time: 5m2


# 2th test 
<strong> change:  -Ddfs.blocksize=512M, container sizes: 1024 and 2048 </strong>
nb mapper decreased to 3 (instead of 8)
Container memory :  1024

* Teragen Time: 2m10 <strong> ==> No change</strong> 

* Terasort Time: 4m16 

Container memory : 2048 

* Teragen Time: 2m42

* Terasort Time: 16m46 <strong> HIGH TIME AND HIGH CLUSTER RESOURCES CONSUMPTION</strong> 
 
# Explanation:

<strong> The teragen time </strong> is almost the same whatever is the size of the container: 512Mo will be upper rounded to 1 Go which is the minimum container size in the cluster.

<strong> The terasort took too much </strong>

Cluster configuration:

* yarn.nodemanager.resource.memory-mb is 4 Go: 

* yarn.scheduler.maximum-allocation-mb is 4 Go 

Given the values of the two above parameters, it means that only 2 containers could run on a node at once, which could explain why we had such slowness with a 2Go container size for reducer in terasort


* yarn.nodemanager.resource.cpu-vcores is 4: which limit number of vcores on a node to 4. or terasort is cpu intensive, increasing this value would improve terasort performance.


